.. _concepts-model:

=====
Model
=====

At a high-level, a :py:class:`~lightning_ir.base.model.LightningIRModel` is a wrapper around a pre-trained `Hugging Face <https://huggingface.co/models/>`_ model that specifies how the embeddings generated by that pre-trained model should be aggregated to compute a relevance score between a query and a document. The behavior of the model is dictated by a :py:class:`~lightning_ir.base.config.LightningIRConfig`. Two types of models are supported: :py:class:`~lightning_ir.bi_encoder.bi_encoder_model.BiEncoderModel` and :py:class:`~lightning_ir.cross_encoder.cross_encoder_model.CrossEncoderModel`, each with their correpsonding :py:class:`~lightning_ir.bi_encoder.bi_encoder_config.BiEncoderConfig` and :py:class:`~lightning_ir.cross_encoder.cross_encoder_config.CrossEncoderConfig`.  

A :py:class:`~lightning_ir.base.model.LightningIRModel` is backbone agnostic and can be used with any pre-trained model from the Hugging Face Model Hub. To initialize a new :py:class:`~lightning_ir.base.model.LightningIRModel`, select a pre-trained model from the `Hugging Face Model Hub <https://huggingface.co/models/>`_, create a :py:class:`~lightning_ir.base.config.LightningIRConfig`, and pass both to the :py:meth:`~lightning_ir.base.model.LightningIRModel.from_pretrained` method. Models already fine-tuned using Lightning IR can be loaded directly without specifying a config. See the :ref:`model-zoo` for a list of pre-trained models.

.. code-block:: python

    from lightning_ir import LightningIRModel, BiEncoderConfig

    model = LightningIRModel.from_pretrained(
        "bert-base-uncased", config=BiEncoderConfig()
    )
    print(type(model))
    # <class 'lightning_ir.base.class_factory.BiEncoderBertModel'>
    
    model = LightningIRModel.from_pretrained(
        "google/electra-base-discriminator", config=CrossEncoderConfig()
    )
    print(type(model))
    # <class 'lightning_ir.base.class_factory.CrossEncoderElectraModel'>



Bi-encoder models compute a relevance score by embedding the query and document separately and compute the similarity between the two embeddings. A cross-encoder receives both the query and document as input and computes a relevance score based on the joint contextualized embedding. See the :ref:`bi-encoder` and :ref:`cross-encoder` sections for more details.

The easiest way to use a :py:class:`~lightning_ir.base.model.LightningIRModel` is through it's corresponding :py:class:`~lightning_ir.base.module.LightningIRModule`. The module combines a :py:class:`~lightning_ir.base.model.LightningIRModel` and :py:class:`~lightning_ir.base.tokenizer.LightningIRTokenizer` and handles the forward pass of the model. The following example illustrates how to use a bi-encoder or a cross-encoder model to score the relevance between a query and a document. Note the bi-encoder generates two embedding tensors while the cross-encoder generates a single joint embedding tensor.

.. literalinclude:: ../../examples/scoring.py
    :language: python
    

.. _bi-encoder:

Bi-Encoder
++++++++++

The :py:class:`~lightning_ir.bi_encoder.bi_encoder_config.BiEncoderConfig` specifies how the contextualized embeddings of a pre-trained Hugging Face model are further processed and how the relevance score is computed based on the embeddings. The processing pipeline includes four steps which are executed in order and can be separately configured for query and document processing: projection, sparsification, pooling, and normalization. The flexibility of this pipeline allows for configuring a plethora of popular bi-encoder models, from learned sparse models like `SPLADE <https://arxiv.org/abs/2107.05720>`_ to dense multi-vector models like `ColBERT <https://arxiv.org/abs/2004.12832>`_. The following sections go over the pipeline stages in detail and how these can be configured.

Backbone Encoding
-----------------

First, an input sequence of tokens (e.g., the query or document), are fed through a pre-trained backbone language model from Hugging Face. The model generates contextualized embeddings, one vector for each token in the input sequence, that is passed to the projection step.

Projection
----------

The projection step scales the dimensionality of the embeddings. Four options are available: ``None``, ``linear``, ``linear_no_bias``, ``mlm``. Setting projection to ``None`` will leave the contextualized embeddings as is. The ``linear`` and ``linear_no_bias`` options project the embeddings using a linear layer. The dimensionality of the resulting embeddings is configured with the ``embedding_dim`` option. For example, if ``embedding_dim`` is set to ``128``, the resulting embedding tensor will have shape ``Sx128``. Finally, the ``mlm`` option uses the pre-trained masked language modeling head of an encoder model to project the embeddings into the dimensionality of the vocabulary size. This is useful for learned sparse models such as SPLADE.

Sparsification
--------------

The parsification step applies applies a function to sparsify the embedding vectors. Three options are available: ``None``, ``relu``, and ``relu_log``. Setting sparsification to ``None`` will leave the contextualized embeddings as is. The ``relu`` and ``relu_log`` options apply a ReLU activation function (and then a logarithm) to set all entries below 0 to 0. This is useful for learned sparse models such as SPLADE.

Pooling
-------

The pooling step aggregates the embedding vectors for every token into a single embedding vector. Five options are available: ``None``, ``first``, ``mean``, ``max``, and ``sum``. Setting pooling to ``None`` will not apply pooling and keep all the embedding vectors of all tokens. This option should be used for multi-vector models such als ColBERT. When setting pooling to ``first`` the model uses the first token's contextualized embedding vector as the aggregated embedding (for models with a BERT-backbone this corresponds to [CLS] pooling). The ``mean``, ``max``, and ``sum`` options aggregate an embedding vector over all tokens' embedding vectors using the respective operator.

Normalization
-------------

Normalization normalizes the embedding vector(s). It can be either ``True`` or ``False``.

Scoring
-------

After embedding both the query and document embeddings, the model computes a relevance score using a scoring function. First, the similarity (either ``dot`` product or ``cosine``) between all query and document embeddings vectors is computed. If pooling was applied, the query and document embeddings consist only of a single vector and their similarity is the final relevance score. If no pooling is applied, the similarity scores are aggregated. First, the scoring function computes the maximum similarity over all document embedding vectors per query embedding vector. Finally, the operator to aggregate over the maximum similarities per query embedding vector is parameterizable in the ``query_aggregation_function`` option. Three options are available: ``sum``, ``mean``, and ``max``.

.. _cross-encoder:

Cross-Encoder
+++++++++++++

The :py:class:`~lightning_ir.cross_encoder.cross_encoder_config.CrossEncoderConfig` specifies how the model further processes the contextualized embeddings of the backbone encoder model to compute a similarity score. A cross-encoder receives both the query and document as input. To compute a relevance score, the model first aggregates the joint contextualized embeddings using a pooling function. Four options are available: ``first``, ``mean``, ``max``, and ``sum``. When setting pooling to ``first`` the model uses the first token's contextualized embedding vector as the aggregated embedding (for models with a BERT-backbone this corresponds to [CLS] pooling). The ``mean``, ``max``, and ``sum`` options aggregate an embedding vector over all tokens' embedding vectors using the respective operator. The model computes the finla relevance score by applying a linear layer to the pooled contextualized embedding vector.